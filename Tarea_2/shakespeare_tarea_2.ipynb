{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar dependencias\n",
    "Para esta tarea, se han agregado algunos requerimientos, asegúrese de instalarlos (puede usar el mismo entorno virtual de la Tarea 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install jupyter pandas \"sqlalchemy<2.0\" pymysql seaborn pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install squarify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "#Parte 1\n",
    "import collections\n",
    "import squarify   \n",
    "import seaborn as sb\n",
    "import nltk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión a la Base y Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\") / \"shakespeare\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_table(table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "print(\"Conectando a la base...\")\n",
    "conn_str = \"mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/Shakespeare\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Todos los párrafos de todas las obras\n",
    "df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "df_works = load_table(\"works\", engine)\n",
    "\n",
    "df_chapters = load_table(\"chapters\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DONE: Version de clean_text() actualizada a la misma usada en la Tarea_1\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # se quitan las indicaciones de escena\n",
    "    result = result.loc[~result.str.startswith(\"[\") & ~result.str.endswith(\"]\")]\n",
    "\n",
    "    # Quitar signos de puntuación y cambiarlos por espacios (\" \")\n",
    "    # TODO: completar signos de puntuación faltantes\n",
    "    for punc in [\"\\n\", \",\", \";\", \".\", \"?\", \"!\", \":\", \"-\",\"--\", \"\\\"\", \"(\", \")\",\"&c\",\"[\",\"]\",]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    \n",
    "    # Cambiar las contracciones por sus palabras ---> TAMBIEN PARA LAS NEGATIVAS?\n",
    "    contractions = [{\"contraction\": \"'re\", \"word\": \"are\"}, {\"contraction\": \"'ll\", \"word\": \"will\"}, \n",
    "                    {\"contraction\": \"'ve\", \"word\": \"have\"}, {\"contraction\": \"'twas\", \"word\": \"it was\"}]    \n",
    "    for c in contractions:\n",
    "        result = result.str.replace(c[\"contraction\"], c[\"word\"])\n",
    "        \n",
    "    # Para las contracciones que no detectamos eliminamos el apostrofe y el palabra abreviada.\n",
    "    \n",
    "    result = result.str.replace(r\"'[^']*?\\s\", \" \", regex=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    #cambiar digitos por sus palabras\n",
    "    digits =[{'digit':'0','word':'zero'},{'digit':'1','word':'one'},{'digit':'2','word':'two'},{'digit':'3','word':'three'},\n",
    "             {'digit':'4','word':'four'},{'digit':'5','word':'five'},{'digit':'6','word':'six'},{'digit':'7','word':'seven'},\n",
    "             {'digit':'8','word':'eight'},{'digit':'9','word':'nine'}]\n",
    "\n",
    "    for d in digits:\n",
    "        result = result.str.replace(d[\"digit\"], d[\"word\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "#Quitamos las filas que fueron limpiadas por completo y ahora estan como NaNs\n",
    "df_dataset[df_dataset['CleanText'].isnull()]\n",
    "df_dataset = df_dataset.dropna()\n",
    "df_dataset[df_dataset['CleanText'].isnull()]\n",
    "\n",
    "df_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Features de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.1. Generar muestreo estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Partir train/test 30% estratificados\n",
    "# Parte 1.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,# Proporcion del conjunto de test \n",
    "                                                    random_state=1,#Semilla para los sorteos aleatorios\n",
    "                                                    stratify=y, #Mantengo la proporcion de personajes\n",
    "                                                   )\n",
    "\n",
    "\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.2. Visualizar el balance de párrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_proporciones(data):\n",
    "    return dict(sorted(zip(collections.Counter(data).keys(), [v for v in collections.Counter(data).values()])))\n",
    "#y.size\n",
    "y_count= count_proporciones(y)\n",
    "y_train_count=count_proporciones(y_train)\n",
    "y_test_count=count_proporciones(y_test)\n",
    "print(y_count)\n",
    "print(y_train_count)\n",
    "print(y_test_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1.2 Genere una visualización que permita verificar que el balance de párrafos de cada\n",
    "\n",
    "# generar los df\n",
    "df_y = pd.DataFrame.from_dict(y_count, orient='index').reset_index().rename(columns={'index':'Personajes',0:'Proporcion'})\n",
    "df_y_train = pd.DataFrame.from_dict(y_train_count, orient='index').reset_index().rename(columns={'index':'Personajes',0:'Proporcion'})\n",
    "df_y_test = pd.DataFrame.from_dict(y_test_count, orient='index').reset_index().rename(columns={'index':'Personajes',0:'Proporcion'})\n",
    "\n",
    "# grafico para ir poniendo los treemap juntos\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# treemap para y\n",
    "squarify.plot(sizes=df_y['Proporcion'], label=df_y['Personajes'] + '\\n'+ df_y['Proporcion'].astype(str),\n",
    "              alpha=.8, color=sb.color_palette(\"coolwarm\", len(df_y['Proporcion'])), ax=axes[0])\n",
    "axes[0].set_title('Proporción Conjunto y', fontsize=18, fontweight=\"bold\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# treemap para y_train\n",
    "squarify.plot(sizes=df_y_train['Proporcion'], label=df_y_train['Personajes'] + '\\n'+ df_y_train['Proporcion'].astype(str),\n",
    "              alpha=.8, color=sb.color_palette(\"coolwarm\", len(df_y_train['Proporcion'])), ax=axes[1])\n",
    "axes[1].set_title('Proporción Conjunto y_train', fontsize=18, fontweight=\"bold\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# treemap para y_test\n",
    "squarify.plot(sizes=df_y_test['Proporcion'], label=df_y_test['Personajes'] + '\\n'+ df_y_test['Proporcion'].astype(str),\n",
    "              alpha=.8, color=sb.color_palette(\"coolwarm\", len(df_y_test['Proporcion'])), ax=axes[2])\n",
    "axes[2].set_title('Proporción Conjunto y_test', fontsize=18, fontweight=\"bold\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "# espaciado para cada grafica\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras y TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.3. Matriz Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.astype(str)\n",
    "#X_train.astype(str).sort()\n",
    "df_dataset[df_dataset['CleanText'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1.3. Transforme el texto de entrenamiento en la representacion de bag or words\n",
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape # frecuencia de cada palabra del conjunto de entrenamiento\n",
    "# Fila = párrafo de la obra, la cantidad de filas es len(X_train)\n",
    "# Columna = palabra, la cantidad de columnas es la cantidad de palabras unicas en X_train\n",
    "# Valor = cantidad de veces que aparece la palabra en el párrafo\n",
    "# La matriz es dispersa porque tenemos muchas celdas en cero ya que hay muchas mas palabras que parrafos, y seguro son muchas las palabras que no estan en un parrafo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar la matriz de bag of words\n",
    "\n",
    "# normalizar los valores de la matriz\n",
    "dense_matrix = X_train_counts.toarray()\n",
    "scaler = MinMaxScaler()\n",
    "normalized_matrix = scaler.fit_transform(dense_matrix)\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(10, 8), )\n",
    "cmap = sb.color_palette(\"Blues\", as_cmap=True)\n",
    "sb.heatmap(normalized_matrix, cmap=cmap)\n",
    "\n",
    "plt.title(\"Mapa de color de la matriz Bag of words\")\n",
    "plt.xlabel(\"Palabras\")\n",
    "plt.ylabel(\"Párrafos\")\n",
    "plt.tick_params(\n",
    "    axis=\"x\",\n",
    "    which=\"both\",\n",
    "    bottom = False,\n",
    "    labelbottom = False\n",
    ") \n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"y\",\n",
    "    which=\"both\",\n",
    "    left = False,\n",
    "    labelleft = False\n",
    ") \n",
    "\n",
    "# Mostrar el mapa de calor\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.4. Matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1.4. Obtenga la matriz TF-IDF\n",
    "\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "X_train_tf.shape # matriz TF-IDF\n",
    "# Fila = párrafo de la obra, la cantidad de filas es len(X_train)\n",
    "# Columna = palabra, la cantidad de columnas es la cantidad de palabras unicas en X_train\n",
    "# Valor = importancia relativa de la palabra en el párrafo, considerando su frecuencia de aparición en el párrafo y como su relevancia en el contexto general\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar la matriz de TF-IDF\n",
    "\n",
    "# normalizar los valores de la matriz\n",
    "dense_matrix = X_train_tf.toarray()\n",
    "scaler = MinMaxScaler()\n",
    "normalized_matrix = scaler.fit_transform(dense_matrix)\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = sb.color_palette(\"YlOrBr\", as_cmap=True)\n",
    "sb.heatmap(normalized_matrix, cmap=cmap)\n",
    "\n",
    "plt.title(\"Mapa de color de la matriz TF-ID\")\n",
    "plt.xlabel(\"Palabras\")\n",
    "plt.ylabel(\"Párrafos\")\n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"x\",\n",
    "    which=\"both\",\n",
    "    bottom = False,\n",
    "    labelbottom = False\n",
    ") \n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"y\",\n",
    "    which=\"both\",\n",
    "    left = False,\n",
    "    labelleft = False\n",
    ") \n",
    "\n",
    "# Mostrar el mapa de calor\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1.5. Componentes PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 1.5.1. Componentes PCA sobre TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductor = PCA(n_components=2)\n",
    "\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "X_train_red.shape\n",
    "# misma cantidad de palabras, pero los datos se colapsaron en dos dimensiones (componentes de PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "plt.xlabel(\"Componente 1 del PCA\")\n",
    "plt.ylabel(\"Componente 2 del PCA\")\n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"x\",\n",
    "    which=\"both\",\n",
    "    bottom = False,\n",
    "    labelbottom = False\n",
    ") \n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"y\",\n",
    "    which=\"both\",\n",
    "    left = False,\n",
    "    labelleft = False\n",
    ") \n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 1.5.1. Componentes PCA sobre TF-IDF con stop words, n-grama 1 y 2, y con ponderador IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# stop_words=stop_words --> filtrar stop words en ingles usando la lista de nltk\n",
    "# ngram_range=(1,2) --> con n-grama 1 y 2, o sea palabras y combinaciones de dos palabras consecutivas en el texto de entrenamiento\n",
    "count_vect_sw_nG12 = CountVectorizer(stop_words=stop_words, ngram_range=(1,2))\n",
    "X_train_counts_sw_nG12 = count_vect_sw_nG12.fit_transform(X_train)\n",
    "\n",
    "# use_idf=True --> con ponderador IDF\n",
    "tf_idf_sw_nG12 = TfidfTransformer(use_idf=True)\n",
    "X_train_tf_sw_nG12 = tf_idf_sw_nG12.fit_transform(X_train_counts_sw_nG12)\n",
    "\n",
    "reductor_sw_nG12 = PCA(n_components=2)\n",
    "X_train_red_sw_nG12 = reductor_sw_nG12.fit_transform(X_train_tf_sw_nG12.toarray())\n",
    "print(\"Modelo entrenado =\", X_train_counts_sw_nG12.shape)\n",
    "print(\"Modelo reducido por PCA =\", X_train_red_sw_nG12.shape)\n",
    "\n",
    "# La cantidad de terminos en este nuevo modelo es de 9409 dado que tenemos n-gramas 1 (palabras sin stopwords) y 2 (dos palabras sin stopwords continuas), mientras que antes eran 2862 (solo las palabras con los stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red_sw_nG12[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje filtrando por stopwords, n-grama (1,2) y con ponderador IDF\")\n",
    "plt.xlabel(\"Componente 1 del PCA\")\n",
    "plt.ylabel(\"Componente 2 del PCA\")\n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"x\",\n",
    "    which=\"both\",\n",
    "    bottom = False,\n",
    "    labelbottom = False\n",
    ") \n",
    "\n",
    "plt.tick_params(\n",
    "    axis=\"y\",\n",
    "    which=\"both\",\n",
    "    left = False,\n",
    "    labelleft = False\n",
    ")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 1.5.3. Componentes PCA sobre TF-IDF con signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ES OPCIONAL, VER SI DA EL TIEMPO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cw = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    # Transformar train\n",
    "    reductor = PCA(n_components=i)\n",
    "    X_train_aux = reductor.fit_transform(X_train_tf.toarray())\n",
    "    var_cw.append(np.var(X_train_aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianza\n",
    "var_sw_nG12 = []\n",
    "\n",
    "for i in range(1, 50):\n",
    "    # Transformar train\n",
    "    reductor = PCA(n_components=i)\n",
    "    X_train_aux = reductor.fit_transform(X_train_tf_sw_nG12.toarray())\n",
    "    var_sw_nG12.append(np.var(X_train_aux))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: arreglar los valores del eje y ponerle un titulo\n",
    "plt.plot(var_sw_nG12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2.1. Modelo Multinomial Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 2.1.1. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bayes_clf = MultinomialNB().fit(X_train_tf_sw_nG12, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las primeras 10 predicciones de train\n",
    "y_pred_train = bayes_clf.predict(X_train_tf_sw_nG12)\n",
    "pd.DataFrame({\"Predccion\":y_pred_train[:10],\"Parrafo\":X_train[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 2.1.2. Predecir sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predecir para test y ver la matriz de confusión, y reportar accuracy\n",
    "\n",
    "X_test_counts_sw_nG12 = count_vect_sw_nG12.transform(X_test)\n",
    "X_test_tf_sw_nG12 = tf_idf_sw_nG12.transform(X_test_counts_sw_nG12)\n",
    "y_test_pred = bayes_clf.predict(X_test_tf_sw_nG12)\n",
    "\n",
    "# accuracy para conjunto de test\n",
    "print(\"Accuracy = \", get_accuracy(y_test, y_test_pred))\n",
    "\n",
    "# matriz de confusion\n",
    "cm_display = ConfusionMatrixDisplay.from_estimator(bayes_clf, X_test_tf_sw_nG12, y_test)\n",
    "plt.title(\"Matriz de confusión\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificacion\n",
    "reporte_clasificacion = classification_report(y_test, y_test_pred)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(reporte_clasificacion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte_clasificacion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper-parámetros con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "testing = False\n",
    "reporte = []\n",
    "# TODO: Agregar más variantes de parámetros que les parezcan relevantes\n",
    "param_sets = [{\"stop_words\": None, \"ngram\": (1,1), \"idf\": False},\n",
    "              {\"stop_words\": None, \"ngram\": (1,1), \"idf\": True},\n",
    "              {\"stop_words\": None, \"ngram\": (1,2), \"idf\": False},\n",
    "              {\"stop_words\": None, \"ngram\": (1,2), \"idf\": True},\n",
    "              {\"stop_words\": stop_words, \"ngram\": (1,1), \"idf\": False},\n",
    "              {\"stop_words\": stop_words, \"ngram\": (1,1), \"idf\": True},\n",
    "              {\"stop_words\": stop_words, \"ngram\": (1,2), \"idf\": False},\n",
    "              {\"stop_words\": stop_words, \"ngram\": (1,2), \"idf\": True}]\n",
    "\n",
    "# Separamos para cada\n",
    "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=1)\n",
    "\n",
    "# Ahora usaremos train/validation/test\n",
    "# Por lo tanto le renombramos train+validation = dev(elopment) dataset\n",
    "X_dev = X_train\n",
    "y_dev = y_train\n",
    "\n",
    "# # Para evitar errores\n",
    "# del X_train\n",
    "# del y_train\n",
    "#Para cada configuracion parametrica\n",
    "for i_params, params in enumerate(param_sets):\n",
    "    #Itero entre los distintos splits, manteniendo el indice que corresponde al conjunto de parametros\n",
    "    for i_folds ,(train_idxs, val_idxs) in enumerate(skf.split(X_dev, y_dev)):\n",
    "        if i_params != i_folds:\n",
    "            # Transformaciones a aplicar (featurizers)\n",
    "            count_vect = CountVectorizer(stop_words=params[\"stop_words\"], ngram_range=params[\"ngram\"])\n",
    "            tf_idf = TfidfTransformer(use_idf=params[\"idf\"])\n",
    "\n",
    "            # Obtengo los elementos de Train para el split actual\n",
    "            X_train_ = X_dev[train_idxs]\n",
    "            y_train_ = y_dev[train_idxs]\n",
    "            if testing:\n",
    "                print({\"X_train_.shape: \": X_train_.shape})\n",
    "                print({\"y_train_.shape: \": y_train_.shape})\n",
    "            \n",
    "            # Ajustamos y transformamos el Train\n",
    "            X_train_counts = count_vect.fit_transform(X_train_)\n",
    "            X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "            if testing:\n",
    "                print({\"X_train_counts.shape: \": X_train_counts.shape})\n",
    "                print({\"X_train_tf.shape: \": X_train_tf.shape})\n",
    "            \n",
    "            # Entrenamos con el Train\n",
    "            bayes_clf = MultinomialNB().fit(X_train_counts, y_train_)\n",
    "            \n",
    "            # Validation para el split actual\n",
    "            X_val_ = X_dev[val_idxs]\n",
    "            y_val_ = y_dev[val_idxs]\n",
    "            if testing:\n",
    "                print({\"X_val_.shape: \": X_val_.shape})\n",
    "                print({\"y_val_.shape: \": y_val_.shape})    \n",
    "\n",
    "            # Transformamos Validation\n",
    "            X_val_counts = count_vect.transform(X_val_)\n",
    "            X_val_tfidf = tf_idf.transform(X_val_counts)\n",
    "            if testing:\n",
    "                print({\"X_val_counts.shape: \": X_val_counts.shape})\n",
    "                print({\"X_val_tfidf.shape: \": X_val_tfidf.shape})\n",
    "                \n",
    "            # Predecimos y evaluamos en Validation\n",
    "            y_pred_val = bayes_clf.predict(X_val_tfidf)\n",
    "            if testing:\n",
    "                print({\"y_pred_val.shape: \": y_pred_val.shape})\n",
    "                print({\"y_val.shape: \": y_val_.shape})\n",
    "            \n",
    "            acc = get_accuracy(y_val_, y_pred_val)\n",
    "            prec = precision_score(y_val_, y_pred_val, average='weighted')\n",
    "            rec = recall_score(y_val_, y_pred_val, average='weighted')\n",
    "            fscore = f1_score (y_val_, y_pred_val, average='weighted')\n",
    "            \n",
    "            reporte_actual=[i_params+1, i_folds+1, acc, prec, rec, fscore]\n",
    "            if testing:\n",
    "                print(reporte_actual)\n",
    "            reporte +=[reporte_actual]\n",
    "\n",
    "df_reporte= pd.DataFrame(data=reporte, columns=['Configuracion', 'Fold','Accuracy', 'Precission','Recall', 'F1Score'])\n",
    "if testing:\n",
    "    print(df_reporte)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reporte.loc[df_reporte['Configuracion']==6]\n",
    "print(\"Promedio de metricas:\")\n",
    "print(df_reporte.groupby(['Configuracion']).mean()[['Accuracy', 'Precission','Recall', 'F1Score']])\n",
    "print(\"Varianza de metricas:\")\n",
    "print(df_reporte.groupby(['Configuracion']).var()[['Accuracy', 'Precission','Recall', 'F1Score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "collectn_1 = np.random.normal(100, 10, 200)\n",
    "collectn_2 = np.random.normal(80, 30, 200)\n",
    "collectn_3 = np.random.normal(90, 20, 200)\n",
    "collectn_4 = np.random.normal(70, 25, 200)\n",
    "\n",
    "## combine these different collections into a list\n",
    "data_to_plot = [df_reporte[df_reporte.Configuracion ==1].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==2].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==3].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==4].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==5].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==6].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==7].F1Score, \n",
    "                df_reporte[df_reporte.Configuracion ==8].F1Score,]\n",
    "\n",
    "# Create a figure instance\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "# Create the boxplot\n",
    "bp = ax.violinplot(data_to_plot)\n",
    "plt.title(\"Variabilidad del F1Score\")\n",
    "plt.xlabel('Configuracion Parametrica') \n",
    "plt.ylabel('F1-Score') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Opcional) Comparativa con Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "y_train_s = np.char.replace(y_train.astype(str), \" \", \"_\").astype(object)\n",
    "y_test_s = np.char.replace(y_test.astype(str), \" \", \"_\").astype(object)\n",
    "\n",
    "# Convertimos al formato de fasttext: archivo de texto donde cada línea es:\n",
    "# __label__<label> TEXTO\n",
    "Xytrains = \"__label__\" + y_train_s.astype(object) + \" \" + X_train\n",
    "Xytests = \"__label__\" + y_test_s.astype(object) + \" \" + X_test\n",
    "np.savetxt(data_dir / \"train.txt\", Xytrains, fmt=\"%s\")\n",
    "np.savetxt(data_dir / \"test.txt\", Xytests, fmt=\"%s\")\n",
    "\n",
    "Xytests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=str(data_dir / \"train.txt\"), epoch=100, wordNgrams=2)\n",
    "model.test(str(data_dir / \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(list(X_test))\n",
    "y_pred_test = [y[0].replace(\"__label__\", \"\") for y in y_out[0]]\n",
    "    \n",
    "print(get_accuracy(y_test_s, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
